{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries\n",
    "\n",
    "\n",
    "In the following code, we are setting up the environment by importing required libraries.\n",
    "- `openml` for accessing datasets from the OpenML repository.\n",
    "- `time` for handling time-related operations.\n",
    "- `jax` for high-performance numerical computing.\n",
    "- `matplotlib.pyplot` for plotting graphs.\n",
    "- Various JAX modules for specific functionalities like gradient computation, random number generation, etc.\n",
    "- `typing` for type hinting.\n",
    "- `IPython.display.Markdown` for displaying Markdown content in IPython.\n",
    "- `tqdm.auto.trange` for creating a tqdm progress bar.\n",
    "\n",
    "The variable `SEED` is set to 404 for seeding random number generators, ensuring reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Union, Tuple, Iterable, List\n",
    "from IPython.display import Markdown\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST Dataset for Machine Learning\n",
    "\n",
    "In this code snippet, we delve into the MNIST dataset using the OpenML library. The MNIST dataset is a collection of handwritten digits widely used in machine learning and computer vision. Let's break down the steps:\n",
    "\n",
    "1. [**Fetching the Dataset:**](#fetching-the-dataset)\n",
    "   We use OpenML to retrieve the MNIST dataset, a crucial resource for developing and testing machine learning models. The dataset comprises images of handwritten digits, each labeled with its corresponding numeric value.\n",
    "\n",
    "2. [**Extracting Features and Labels:**](#extracting-features-and-labels)\n",
    "   With the fetched dataset, we extract the features (X) and labels (y). The features represent the pixel values of the handwritten digits, while the labels indicate the numeric values they represent. This step is fundamental for training machine learning models.\n",
    "\n",
    "3. [**Data Preparation:**](#data-preparation)\n",
    "   To ensure data compatibility, we convert the label data to integers. This step is essential for downstream tasks like model training, where proper data types are crucial.\n",
    "\n",
    "The provided code offers a concise way to access and understand the MNIST dataset, laying the groundwork for future machine learning endeavors. Stay tuned for further exploration and analysis!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset ' MNIST '\n",
    "dataset = openml.datasets.get_dataset(dataset_id=554, \n",
    "                                      download_data=False, \n",
    "                                      download_qualities=False, \n",
    "                                      download_features_meta_data=False)\n",
    "Markdown(dataset.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and target variable (y) from the MNIST dataset\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute, \n",
    "                                    dataset_format=\"dataframe\")\n",
    "\n",
    "# Ensure the target variable is of integer type\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Feature shape:{X.shape}')\n",
    "print(f'Label shape:{y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of examples for training and testing follow dataset.description\n",
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "# Split the data manually\n",
    "X_train, X_test = X.iloc[:train_size, :].to_numpy(), X.iloc[train_size:train_size + test_size, :].to_numpy()\n",
    "y_train, y_test = y.iloc[:train_size].to_numpy(), y.iloc[train_size:train_size + test_size].to_numpy()\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Neural Network with JAX\n",
    "\n",
    "In this step, we implement a simple neural network using the JAX library, a powerful tool for high-performance numerical computing. Let's break down the code:\n",
    "\n",
    "### 1. Initializing Network Parameters\n",
    "   - `random_layer_params(m, n, key, scale=1e-2)`: Helper function to randomly initialize weights and biases for a dense neural network layer.\n",
    "   - `init_network_params(sizes, key)`: Initializes all layers for a fully-connected neural network with specified sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2) -> Tuple:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - m (int): Number of input neurons.\n",
    "    - n (int): Number of output neurons.\n",
    "    - key (jax.random.PRNGKey): Random key for reproducibility.\n",
    "    - scale (float): Scaling factor for weight and bias initialization.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: Tuple containing weight and bias arrays.\n",
    "    \"\"\"\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - sizes (List[int]): List of layer sizes.\n",
    "    - key (jax.random.PRNGKey): Random key for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - List[Tuple]: List of tuples containing weight and bias arrays for each layer.\n",
    "    \"\"\"\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Activation Function\n",
    "   - `relu(x)`: Rectified Linear Unit (ReLU) activation function applied element-wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - x (array): Input array.\n",
    "\n",
    "    Returns:\n",
    "    - array: Output array after applying ReLU activation function.\n",
    "    \"\"\"\n",
    "    return jnp.maximum(0, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prediction and Batch Processing\n",
    "   - `predict(params, image)`: Makes per-example predictions using the initialized parameters.\n",
    "   - `batched_predict(params, images)`: A batched version of the `predict` function using JAX's `vmap` for efficient processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp # Compute the log of the sum of exponentials of input elements\n",
    "def predict(params, image):\n",
    "    \"\"\"\n",
    "    Predicts output given input using a fully-connected neural network.\n",
    "\n",
    "    Args:\n",
    "    - params (List[Tuple]): List of tuples containing weight and bias arrays for each layer.\n",
    "    - image (array): Input data.\n",
    "\n",
    "    Returns:\n",
    "    - array: Predicted logits.\n",
    "    \"\"\"\n",
    "    # per-example predictions\n",
    "    activations = image\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "  \n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. One-Hot Encoding\n",
    "   - `one_hot(x, k)`: Creates a one-hot encoding of input labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k):\n",
    "    \"\"\"\n",
    "    Create a one-hot encoding of x of size k.\n",
    "\n",
    "    Args:\n",
    "    - x (array): Input array of integer labels.\n",
    "    - k (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - array: One-hot encoded array.\n",
    "    \"\"\"\n",
    "    return jax.nn.one_hot(x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation Metrics\n",
    "   - `accuracy(params, images, targets)`: Computes accuracy of predictions.\n",
    "   - `loss_fn(params, images, targets)`: Computes the cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, images, targets):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predictions.\n",
    "\n",
    "    Args:\n",
    "    - params (List[Tuple]): List of tuples containing weight and bias arrays for each layer.\n",
    "    - images (array): Input data.\n",
    "    - targets (array): Target labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy.\n",
    "    \"\"\"\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss_fn(params, images, targets):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "    - params (List[Tuple]): List of tuples containing weight and bias arrays for each layer.\n",
    "    - images (array): Input data.\n",
    "    - targets (array): Target labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Parameter Update\n",
    "   - `update(params, x, y, lr=1e-4)`: Updates model parameters using gradient descent. The `@jit` decorator optimizes the function for performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Update the model parameters using gradient descent.\n",
    "\n",
    "    Args:\n",
    "    - params (List[Tuple]): List of tuples containing weight and bias arrays for each layer.\n",
    "    - x (array): Input data.\n",
    "    - y (array): Target labels.\n",
    "    - lr (float): Learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    - List[Tuple]: Updated model parameters.\n",
    "    \"\"\"\n",
    "    grads = grad(loss_fn)(params, x, y)\n",
    "    return [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions collectively form the core components for constructing, training, and evaluating a neural network. The code emphasizes modularity, making it adaptable for various machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Minibatches with JAX for Efficient Training\n",
    "\n",
    "In this step, we implement a function to generate minibatches from given JAX tensors, a crucial practice for efficient training of machine learning models. Let's break down the code:\n",
    "\n",
    "### `get_minibatch` Function\n",
    "   - **Parameters:**\n",
    "      - `*tensors (jnp.ndarray)`: Variable number of JAX arrays representing input data.\n",
    "      - `minibatch_size (int)`: Size of each minibatch. Default is set to 1000.\n",
    "   - **Yields:**\n",
    "      - `Tuple[jnp.ndarray, ...]`: A tuple containing minibatches of input tensors.\n",
    "\n",
    "### Function Description\n",
    "   The `get_minibatch` function takes JAX tensors representing input data and generates minibatches of specified sizes. It uses JAX's random module to shuffle indices for each minibatch, ensuring randomness and avoiding bias during training. The function yields tuples of minibatches, making it suitable for efficient model training with large datasets.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "# Example usage with two input tensors X and y\n",
    "for minibatch_X, minibatch_y in get_minibatch(X, y, minibatch_size=128):\n",
    "    # Perform model training or evaluation on minibatch\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(*tensors: np.ndarray, \n",
    "                  minibatch_size: int = 1000) -> Iterable[Tuple[jnp.ndarray, ...]]:\n",
    "    \"\"\"\n",
    "    Generate minibatches from given JAX tensors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensors (jnp.ndarray): Variable number of JAX arrays representing input data.\n",
    "    - minibatch_size (int): Size of each minibatch. Default is 1000.\n",
    "\n",
    "    Yields:\n",
    "    - Tuple[jnp.ndarray, ...]: A tuple containing minibatches of input tensors.\n",
    "    \"\"\"\n",
    "    n = tensors[0].shape[0]\n",
    "\n",
    "    # Check that all tensors have the same number of samples\n",
    "    for tensor in tensors:\n",
    "        assert tensor.shape[0] == n, \"All tensors must have the same number of samples.\"\n",
    "\n",
    "    # Randomly shuffle indices using JAX's random module\n",
    "     # Randomly shuffle indices\n",
    "    key = jax.random.PRNGKey(SEED)  # Use your desired PRNG key\n",
    "    idx = jax.random.permutation(key, jnp.arange(n), independent=True)\n",
    "\n",
    "\n",
    "    # Calculate the number of minibatches\n",
    "    n_minibatch = int(jnp.ceil(n / minibatch_size))\n",
    "\n",
    "    # Generate minibatches\n",
    "    for i in range(n_minibatch):\n",
    "        bidx = idx[i * minibatch_size: (i + 1) * minibatch_size]\n",
    "        yield tuple(jnp.array(t[bidx], dtype=jnp.float32)  for t in tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code promotes efficient training by allowing the model to learn from subsets of the data in each iteration, facilitating convergence and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Parameters and Initializing Model\n",
    "\n",
    "Now that we have essential functions for training our neural network, let's set up the parameters and initialize the model.\n",
    "\n",
    "### Model Architecture\n",
    "We define the layer sizes of our neural network, representing the number of neurons in each layer. For example, a feedforward neural network with 784 input neurons, two hidden layers of 512 neurons each, and an output layer with 10 neurons:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [784, 512, 512, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters\n",
    "We set the number of training epochs, batch size, and the number of target classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters\n",
    "Now, we initialize the model parameters using the defined layer sizes and a random key for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_network_params(layer_sizes, random.PRNGKey(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters, along with the functions we've implemented, lay the foundation for training our neural network. Adjust these values based on your specific requirements or experimentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Training and Test Datasets\n",
    "\n",
    "In this section, we load and prepare the datasets for training and testing our neural network. We convert the raw data into JAX arrays and perform one-hot encoding for the target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = jnp.array(X_train)\n",
    "train_labels = one_hot(y_train, n_targets)\n",
    "\n",
    "test_images = jnp.array(X_test, dtype=jnp.float32)\n",
    "test_labels = one_hot(y_test, n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "In this section, we train our neural network using mini-batch stochastic gradient descent. The training loop runs for a specified number of epochs, and for each epoch, we update the model parameters based on mini-batches of the training data.\n",
    "\n",
    "```python\n",
    "Input: \n",
    "    learning_rate (γ)\n",
    "    max_epochs\n",
    "    model_params (θ)\n",
    "    objective_function (f(θ))\n",
    "    data_loader (providing minibatches)\n",
    "\n",
    "for epoch in 1 to max_epochs do\n",
    "    start_time = current_time()\n",
    "    \n",
    "    for minibatch_inputs, minibatch_outputs in data_loader do\n",
    "        # Compute gradients\n",
    "        gradients = ∇f(θ, minibatch_inputs, minibatch_outputs)\n",
    "\n",
    "        # Update model parameters\n",
    "        θ = θ - γ * gradients\n",
    "\n",
    "    epoch_time = current_time() - start_time\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in trange(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iterate through minibatches of the training data\n",
    "    for inputs, outputs in get_minibatch(X_train, y_train, minibatch_size=batch_size):\n",
    "        outputs = one_hot(outputs, n_targets)\n",
    "        params = update(params, inputs, outputs, lr=0.01)\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate training and test set accuracy\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch+1, epoch_time))\n",
    "    print(\"Training set accuracy: {:0.4f}\".format(train_acc))\n",
    "    print(\"Test set accuracy: {:0.4f} \\n\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction and Visualization\n",
    "\n",
    "In this section, we select an example from the test dataset, make a prediction using the trained model parameters, and visualize the input image along with the prediction and ground truth label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Ground Truth\n",
    "We choose an index (`idx`) from the test dataset and extract the corresponding image and ground truth label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an index from the test dataset\n",
    "idx = 69\n",
    "\n",
    "# Extract image and ground truth label\n",
    "img = test_images[idx].reshape((28, 28))\n",
    "gt_lbl = test_labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model Prediction\n",
    "Using the trained neural network, we make a prediction for the selected image and print both the predicted class and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using the trained model parameters\n",
    "pred = jnp.argmax(predict(params, np.ravel(img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Visualization\n",
    "Finally, we visualize the input image using Matplotlib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the prediction and ground truth\n",
    "print('Prediction:', pred)\n",
    "print('Ground Truth:', gt_lbl)\n",
    "\n",
    "# Visualize the input image\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code snippet serves as a visual demonstration of the model's prediction capabilities and allows us to compare the predicted class with the actual ground truth label."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "poetry-kernel",
   "name": "common-cpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m114"
  },
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
